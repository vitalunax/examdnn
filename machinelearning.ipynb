{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### NECESSARY EXPLANATIONS\n",
        "\n",
        "i had to create another model that passed the concatanted model because i could not train the model on a layer only that had the outputs so i made a final model with only a one dense layer that passed the concatanated layer as input and the output layer with 2 neurons and the necessary activation function as the output for the final model. that is the final output layer that i consider for my model."
      ],
      "metadata": {
        "id": "iV6j8YFlnuzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LOADING NECESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "ts9S5dXme_5C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gHgRTgaRhNE",
        "outputId": "1116f747-c1c8-4c4d-91e8-3b880055fe04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.x (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.x\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPFHbEGURj2I",
        "outputId": "aafbb56e-4c4d-42e0-aeac-552de1685ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj7LpKDJRl9j",
        "outputId": "f29e04ae-46a2-48d9-99bc-b8ab2600b085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.12.0\n"
          ]
        }
      ],
      "source": [
        "! pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1R38TKSmRpKn"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import scipy as sp\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import scipy as sp\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import f1_score, matthews_corrcoef, confusion_matrix\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, StratifiedKFold, RepeatedKFold, ShuffleSplit, StratifiedShuffleSplit, learning_curve, validation_curve\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, roc_curve\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import scipy as sp\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder, KBinsDiscretizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmVhDavjRrKY"
      },
      "source": [
        "LOADING THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTc9m0ziRsxK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ZIP data URL from GitHub\n",
        "zip_url = \"https://github.com/vitalunax/examdnn/blob/main/input%20(3).zip?raw=true\"\n",
        "\n",
        "# Download the ZIP\n",
        "zip_path, _ = urllib.request.urlretrieve(zip_url)\n",
        "\n",
        "# Unzip the folder\n",
        "target_folder = \"temp\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_folder)\n",
        "\n",
        "data_path = f\"{target_folder}/input.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j_XDATnRwnt"
      },
      "outputs": [],
      "source": [
        "import pickle as pk\n",
        "\n",
        "# Open the pickle data\n",
        "with open(data_path, 'rb') as f:\n",
        "  train_labels, test_labels, train_texts, test_texts, train_bow, test_bow = pk.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxz1RFA6Rz-R"
      },
      "source": [
        "\n",
        "### PREPROCESSING DATA AND PREPARING INPUT\n",
        "\n",
        "\n",
        "> Q2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF8FJ8ev8QVq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " the labels are one hot encoded to be used in training."
      ],
      "metadata": {
        "id": "Fdh6jyqhbz6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hm8GXClwR8QI",
        "outputId": "86550995-9303-41c9-a0e3-baee34440e97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(140, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_ohe = encoder.fit_transform(np.array(train_labels).reshape(-1, 1))\n",
        "test_labels_ohe = encoder.fit_transform(np.array(test_labels).reshape(-1, 1))\n",
        "test_labels_ohe.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i defined early stopping because the grid search and training took a lot of time and put a  lot of weight on the hardware"
      ],
      "metadata": {
        "id": "dwtVa2z5blmo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn_V1CJc8NjR"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,           # Log when training is stopped\n",
        "    mode='min',          # The direction is automatically inferred if not set, but 'min' means we want to minimize validation loss\n",
        "    restore_best_weights=True  # Restores model weights from the epoch with the minimum validation loss\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we have the text data.I did pre processing by first tokenizing and then padding to later on make the model learn the word embeddings and then I scaled it"
      ],
      "metadata": {
        "id": "Ay7cqdhHbjyx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0vWyDOFR0-K"
      },
      "outputs": [],
      "source": [
        "# Preprocessing for text data\n",
        "\n",
        "max_words = 10000\n",
        "embedding_dim = 300\n",
        "max_len = 1302\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post', truncating='post')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZozeTQ_Hd6ZM"
      },
      "outputs": [],
      "source": [
        "    max_len_text_data = 1302  # Adjust this based on your actual data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL STRUCTURING\n",
        "\n",
        "\n",
        "\n",
        "> Q2\n",
        "Q5\n",
        "Q3\n",
        "\n",
        "Here we have two different inputs in which I use both of them with the labels for their own intended model pipe line\n",
        "\n",
        "Here it is also visable that i have used the bianry cross entropy loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q9blmSLcJ6rV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm2Va3QAR_27"
      },
      "outputs": [],
      "source": [
        "def create_model1(optimizer='adam', activation='ReLU',out_act='softmax', loss='binary_crossentropy', batch_size=32, learning_rate=0.001, initializer='glorot_uniform', weight_reg=l1(0.01)):\n",
        "    # Define your Keras model architecture here\n",
        "    max_len_text_data = 1302  # Adjust this based on your actual data\n",
        "    model1 = Sequential()\n",
        "    model1.add(Input(shape=(max_len_text_data,)))\n",
        "    model1.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len_text_data))\n",
        "    model1.add(Flatten())\n",
        "    model1.add(Dense(128, activation=activation, kernel_initializer=initializer, kernel_regularizer=weight_reg))\n",
        "    model1.add(Dense(64, activation=activation, kernel_initializer=initializer, kernel_regularizer=weight_reg))\n",
        "    model1.add(Dense(2, activation=out_act))\n",
        "\n",
        "    model1.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSl8F8tcTeuJ"
      },
      "source": [
        "BOW DATA\n",
        "\n",
        "\n",
        "> Q2\n",
        "Q5\n",
        "\n",
        "\n",
        "here we have BOW data which I applied smoothing to deal with the problem of large amount of zero values after smoothing i applied scaling to ensure that the values are between the needed values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5OGXEb5UcWFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU7w8nYAdpuA"
      },
      "outputs": [],
      "source": [
        "input_size_bow_frequency = 12854\n",
        "# Define the smoothing factor (you can adjust this based on your needs)\n",
        "smoothing_factor = 1e-5\n",
        "\n",
        "# Scale the BoW vectors using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "train_bow_scaled = scaler.fit_transform(train_bow)\n",
        "test_bow_scaled = scaler.transform(test_bow)\n",
        "\n",
        "# Apply additive smoothing to handle zero-values\n",
        "train_bow_smoothed = train_bow_scaled + smoothing_factor\n",
        "test_bow_smoothed = test_bow_scaled + smoothing_factor\n",
        "\n",
        "# Ensure that the values are within the desired range after smoothing\n",
        "train_bow_smoothed = np.clip(train_bow_smoothed, 0, 1)\n",
        "test_bow_smoothed = np.clip(test_bow_smoothed, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Q2 Q5 Q3\n",
        "\n",
        "\n",
        "\n",
        "Here you can see that i HAVE CREATED TWO DIFFERENT MODELS WITH SEVERAL FULLY CONNECTED DENDSE LAYERS WITH RELU ACTIVATION FUNCTION AND AS A FINAL LAYER TWO NEURONS AND SOFMAX ACTIVATION FUNCTION.\n",
        "\n",
        "As an initilizer I used xavier glorot and as regulariser L1 to deal with the sparsity problem and\n",
        "\n",
        "you can see that i have defined each model as their own pipeline\n",
        "\n",
        "and also i have used binary cross entropy as loss function"
      ],
      "metadata": {
        "id": "dCsYxVB9cXBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSIupKSkTgWp"
      },
      "outputs": [],
      "source": [
        "def create_model2(optimizer='adam', activation='ReLU',out_act='softmax', loss='binary_crossentropy', batch_size=32, learning_rate=0.001, initializer='glorot_uniform', weight_reg=l1(0.01)):\n",
        "\n",
        "\n",
        "  # Define the size of BoW frequency data\n",
        "  input_size_bow_frequency = 12854\n",
        "\n",
        "  # Create a Sequential model for BoW frequency data\n",
        "  model2 = Sequential()\n",
        "  model2.add(Input(shape=(input_size_bow_frequency,)))\n",
        "  model2.add(Dense(128, activation=activation, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=l1(0.01)))\n",
        "  model2.add(Dense(64, activation=activation, kernel_initializer='glorot_uniform', bias_initializer='zeros',kernel_regularizer=l1(0.01)))\n",
        "  model2.add(Dense(2, activation=out_act))  # Final layer before the merge\n",
        "\n",
        "\n",
        "\n",
        "  # Compile the merged model\n",
        "  model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=10**-2,\n",
        "                                                 clipvalue=0.5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  return model2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKUOIAZb8c_S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Q5\n",
        "Q4\n",
        "Q3\n",
        "\n",
        "\n",
        "OUTPUT LAYER\n",
        "LOSS FUNCTION\n",
        "\n"
      ],
      "metadata": {
        "id": "ymQqT8AfKPxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "after creating the 2 separate models i have created a one last model with 1 dense layer taking the concateanted layer as input and passing them through to later on use it in training and grid search. and here the only layer that is my intended output layer has again 2 neurons with softmax activation that will have the merged probabilites of the 2 models in the concatanted layer.\n",
        "\n",
        "you can also see the loss function as binary cross entropy here"
      ],
      "metadata": {
        "id": "S3f1ACuqc8f-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egpmjgLxyXdh"
      },
      "outputs": [],
      "source": [
        "def create_model(optimizer='adam', activation='relu', loss='binary_crossentropy', learning_rate=0.01, neurons=2, layers=1):\n",
        "  model1 = create_model1(learning_rate=0.01, loss='binary_crossentropy', activation='ReLU', out_act='softmax',  weight_reg=l1(0.01))\n",
        "  model2 = create_model2(learning_rate=0.01, loss='binary_crossentropy', activation='ReLU', out_act='softmax', weight_reg=l1(0.01))\n",
        "  concatenated_layer = Concatenate(name='concatenated_layer')([model1.layers[-1].output, model2.layers[-1].output])\n",
        "  output = Dense(2, activation='softmax')(concatenated_layer)  # Define the appropriate shape and activatio\n",
        "  model1_input = model1.layers[0].input\n",
        "  model2_input = model2.layers[0].input\n",
        "  model = Model(inputs=[model1_input, model2_input], outputs=output)\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzjtJxpnzhwn",
        "outputId": "3485d2db-3487-44b2-8b38-f85aa06ea3a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.functional.Functional at 0x7d8b9aafa6e0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "model = create_model()\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXNdOb69l3D2",
        "outputId": "64fc4078-ff9d-44b7-8c48-5185fc6a14dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "print(type(train_padded))\n",
        "print(type(train_bow_smoothed))\n",
        "print(type(train_labels_ohe))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Q6\n",
        "\n"
      ],
      "metadata": {
        "id": "9Qc0YO7kKdcQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        ">Q5\n",
        "\n",
        "TRAINING THE MODEL ON THE SELECTED HYPERPARAMETERS\n",
        "\n"
      ],
      "metadata": {
        "id": "-hkSEfGrgFWk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i trained my model and made predictions on it. i have only 30 epochs because again it took a lot of time. I have defined batchsize validation split and 30 epochs."
      ],
      "metadata": {
        "id": "UKZ8gsfHdeA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BolMTN-XSas",
        "outputId": "ab1a57fa-80f0-45b0-a515-bf5549332324"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "14/14 [==============================] - 51s 3s/step - loss: 434.2068 - accuracy: 0.5692 - val_loss: 213.5183 - val_accuracy: 0.2232\n",
            "Epoch 2/30\n",
            "14/14 [==============================] - 30s 2s/step - loss: 152.1018 - accuracy: 0.6295 - val_loss: 121.3521 - val_accuracy: 0.7768\n",
            "Epoch 3/30\n",
            "14/14 [==============================] - 30s 2s/step - loss: 99.8573 - accuracy: 0.6496 - val_loss: 85.2351 - val_accuracy: 0.7768\n",
            "Epoch 4/30\n",
            "14/14 [==============================] - 29s 2s/step - loss: 81.0196 - accuracy: 0.6942 - val_loss: 76.0946 - val_accuracy: 0.7768\n",
            "Epoch 5/30\n",
            "14/14 [==============================] - 32s 2s/step - loss: 73.7392 - accuracy: 0.6942 - val_loss: 74.4379 - val_accuracy: 0.7768\n",
            "Epoch 6/30\n",
            "14/14 [==============================] - 31s 2s/step - loss: 69.7655 - accuracy: 0.6942 - val_loss: 69.2735 - val_accuracy: 0.7768\n",
            "Epoch 7/30\n",
            "14/14 [==============================] - 30s 2s/step - loss: 66.0965 - accuracy: 0.6942 - val_loss: 63.6823 - val_accuracy: 0.7768\n",
            "Epoch 8/30\n",
            "14/14 [==============================] - 29s 2s/step - loss: 63.4696 - accuracy: 0.6942 - val_loss: 63.6104 - val_accuracy: 0.7768\n",
            "Epoch 9/30\n",
            "14/14 [==============================] - 30s 2s/step - loss: 61.7087 - accuracy: 0.6942 - val_loss: 60.8520 - val_accuracy: 0.7768\n",
            "Epoch 10/30\n",
            "14/14 [==============================] - 32s 2s/step - loss: 59.7871 - accuracy: 0.6942 - val_loss: 59.9669 - val_accuracy: 0.7768\n",
            "Epoch 11/30\n",
            "14/14 [==============================] - 34s 2s/step - loss: 58.8028 - accuracy: 0.6942 - val_loss: 60.1191 - val_accuracy: 0.7768\n",
            "Epoch 12/30\n",
            "14/14 [==============================] - 28s 2s/step - loss: 58.1012 - accuracy: 0.6942 - val_loss: 57.8788 - val_accuracy: 0.7768\n",
            "Epoch 13/30\n",
            "14/14 [==============================] - 30s 2s/step - loss: 57.7112 - accuracy: 0.6942 - val_loss: 56.8492 - val_accuracy: 0.7768\n",
            "Epoch 14/30\n",
            "14/14 [==============================] - 31s 2s/step - loss: 57.5057 - accuracy: 0.6942 - val_loss: 58.9434 - val_accuracy: 0.7768\n",
            "Epoch 15/30\n",
            "14/14 [==============================] - 31s 2s/step - loss: 57.5314 - accuracy: 0.6942 - val_loss: 58.3005 - val_accuracy: 0.7768\n",
            "Epoch 16/30\n",
            "14/14 [==============================] - ETA: 0s - loss: 57.3529 - accuracy: 0.6942Restoring model weights from the end of the best epoch: 13.\n",
            "14/14 [==============================] - 30s 2s/step - loss: 57.3529 - accuracy: 0.6942 - val_loss: 57.9999 - val_accuracy: 0.7768\n",
            "Epoch 16: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d8ba2608dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Training\n",
        "model.fit([train_padded, train_bow_smoothed], train_labels_ohe,\n",
        "    validation_split=0.2,  # Assuming you have a validation split\n",
        "    epochs=30,  # Set to a higher number since early stopping will halt training\n",
        "    callbacks=[early_stopping],\n",
        "    batch_size=32,  # Example batch size, adjust according to your needs\n",
        "    verbose=1\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUixTx75mPAe",
        "outputId": "d16d7a3f-59fb-4272-c00c-2fff74bd83fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 120ms/step\n"
          ]
        }
      ],
      "source": [
        "# Prediction\n",
        "predictions = model.predict([test_padded, test_bow_smoothed])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i printed out the shapes for debugging reasons"
      ],
      "metadata": {
        "id": "eKoXmo3idtOG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jF6_5RX2pDXs",
        "outputId": "f192b24e-b395-496d-a80c-d82b6a9227c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test_padded: (140, 1302)\n",
            "Shape of test_bow_smoothed: (140, 12854)\n",
            "Shape of test_labels_ohe: (140, 2)\n"
          ]
        }
      ],
      "source": [
        "print(\"Shape of test_padded:\", test_padded.shape)\n",
        "print(\"Shape of test_bow_smoothed:\", test_bow_smoothed.shape)\n",
        "print(\"Shape of test_labels_ohe:\", test_labels_ohe.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i wrapped the model in keras classifier to perform grid search later on"
      ],
      "metadata": {
        "id": "NVCpTDf4dxCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "kTd5pdbwztWT",
        "outputId": "1ce3d247-ce8c-4d15-d0fa-c44f4a8ba4c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=<function create_model at 0x7d8ba266e440>\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=0\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              "\tclass_weight=None\n",
              ")"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=&lt;function create_model at 0x7d8ba266e440&gt;\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=0\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              "\tclass_weight=None\n",
              ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasClassifier</label><div class=\"sk-toggleable__content\"><pre>KerasClassifier(\n",
              "\tmodel=None\n",
              "\tbuild_fn=&lt;function create_model at 0x7d8ba266e440&gt;\n",
              "\twarm_start=False\n",
              "\trandom_state=None\n",
              "\toptimizer=rmsprop\n",
              "\tloss=None\n",
              "\tmetrics=None\n",
              "\tbatch_size=None\n",
              "\tvalidation_batch_size=None\n",
              "\tverbose=0\n",
              "\tcallbacks=None\n",
              "\tvalidation_split=0.0\n",
              "\tshuffle=True\n",
              "\trun_eagerly=False\n",
              "\tepochs=1\n",
              "\tclass_weight=None\n",
              ")</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Wrap the model so it can be used by scikit-learn\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MODEL SELECTION\n"
      ],
      "metadata": {
        "id": "AkTjK_kBggNF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBlfnLAi3NbK"
      },
      "source": [
        "hyperparameter tuning and model selection\n",
        "\n",
        "\n",
        "\n",
        "> Q5 Q6\n",
        "\n",
        "i couldnt perform grid search on the combined model so i performed it on separately on each model and then extracted the best model out of those.\n",
        "\n",
        "the hyperparameters are the ones included in the create model functions. the initializers and regularizers are also visable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKvNTWi73Pf3"
      },
      "outputs": [],
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "model1_clf = KerasClassifier(activation='relu',learning_rate=0.001, model=create_model1, verbose=0)\n",
        "model2_clf = KerasClassifier(activation='relu',learning_rate=0.001, model=create_model2, verbose=0)\n",
        "param_grid = {\n",
        "    'optimizer': ['adam', 'sgd'],\n",
        "    'activation': ['relu', 'sigmoid'],\n",
        "    'loss': ['binary_crossentropy'],\n",
        "    'learning_rate': [0.001, 0.01],\n",
        "    'batch_size': [32, 64],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeyfCtkh6dex"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Grid search for model1\n",
        "grid1 = GridSearchCV(estimator=model1_clf, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy')\n",
        "grid_result1 = grid1.fit(test_padded, test_labels_ohe)  # Assuming X_train, y_train are defined\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model1 = grid1.best_estimator_\n",
        "best_accuracy1 = grid1.best_score_\n",
        "print(f\"Best Accuracy (Validation) for Model 1: {best_accuracy1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdpgmxLQP4F9",
        "outputId": "764731bd-6af2-40c0-e736-9622cf4d33cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy (Validation) for Model 1: 0.72895467160037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh5BKXkr3YLz",
        "outputId": "56d474eb-ddb0-4fbb-84e8-3cb64241b20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Grid search for model2\n",
        "grid2 = GridSearchCV(estimator=model2_clf, param_grid=param_grid, n_jobs=-1, cv=3, scoring='accuracy')\n",
        "grid_result2 = grid2.fit(test_bow_smoothed, test_labels_ohe)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model2 = grid2.best_estimator_\n",
        "best_accuracy2 = grid2.best_score_\n",
        "print(f\"Best Accuracy (Validation) for Model 2: {best_accuracy2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBPguTc0QRca",
        "outputId": "c2f2ced0-5990-4126-b9b4-14f67d017a9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy (Validation) for Model 2: 0.72895467160037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "here i printed the evaluation metrics which seem completely off which i assume to be because of the complexity of the model i preferred. and i believe i have some other problems that i couldnt seem to figure out why. but as a result i have made predictions on the test data to evaluate the generalization capabilities of my model."
      ],
      "metadata": {
        "id": "wsV0CJiJeKq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming 'X_test', 'y_test' is your test dataset\n",
        "y1_pred = best_model1.predict(test_padded)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(test_labels_ohe, y1_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(test_labels_ohe, y1_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtTa5s4APrI6",
        "outputId": "f6e3b853-8abc-4293-f02a-c5fd47ddf3bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7285714285714285\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        38\n",
            "           1       0.73      1.00      0.84       102\n",
            "\n",
            "   micro avg       0.73      0.73      0.73       140\n",
            "   macro avg       0.36      0.50      0.42       140\n",
            "weighted avg       0.53      0.73      0.61       140\n",
            " samples avg       0.73      0.73      0.73       140\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Assuming 'X_test', 'y_test' is your test dataset\n",
        "y2_pred = best_model2.predict(test_bow_smoothed)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(test_labels_ohe, y2_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(test_labels_ohe, y2_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH7CdqgsQtla",
        "outputId": "78a1ef84-e681-4bee-9ec3-3163bb02b8cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7285714285714285\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        38\n",
            "           1       0.73      1.00      0.84       102\n",
            "\n",
            "   micro avg       0.73      0.73      0.73       140\n",
            "   macro avg       0.36      0.50      0.42       140\n",
            "weighted avg       0.53      0.73      0.61       140\n",
            " samples avg       0.73      0.73      0.73       140\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}